{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79159cb0",
   "metadata": {},
   "source": [
    "# ðŸ” QualiVault: Validate Transcripts\n",
    "**Goal:** Use Ollama (local LLM) to detect transcription errors and hallucinations.\n",
    "\n",
    "1. Loads transcripts from CSV files.\n",
    "2. Samples segments and sends them to Ollama for validation.\n",
    "3. Flags potential errors: hallucinations, misheard words, artifacts.\n",
    "4. Generates validation report with suggestions.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ollama must be installed and running (`ollama serve`)\n",
    "- Install a model: `ollama pull llama3.1` or `ollama pull jobautomation/OpenEuroLLM-Danish:latest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6364fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking Ollama installation...\n",
      "\n",
      "âœ… Ollama is running!\n",
      "\n",
      "ðŸ“¦ Available models (1):\n",
      "\n",
      "   ðŸ¦™ llama3.1:latest (4.6 GB) â† Good general model\n",
      "\n",
      "ðŸ’¡ Recommendation:\n",
      "   Use: 'llama3.1:latest' (general purpose)\n"
     ]
    }
   ],
   "source": [
    "# 1. Check Ollama Installation and Available Models\n",
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama():\n",
    "    \"\"\"Test if Ollama is running and list available models.\"\"\"\n",
    "    print(\"ðŸ” Checking Ollama installation...\\n\")\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Ollama is running!\")\n",
    "            \n",
    "            # List available models\n",
    "            data = response.json()\n",
    "            models = data.get('models', [])\n",
    "            \n",
    "            if models:\n",
    "                print(f\"\\nðŸ“¦ Available models ({len(models)}):\\n\")\n",
    "                \n",
    "                for model in models:\n",
    "                    name = model.get('name', 'Unknown')\n",
    "                    size_gb = model.get('size', 0) / (1024**3)\n",
    "                    \n",
    "                    # Highlight Danish model\n",
    "                    if 'danish' in name.lower() or 'openeuro' in name.lower():\n",
    "                        print(f\"   ðŸ‡©ðŸ‡° {name} ({size_gb:.1f} GB) â† Recommended for Danish\")\n",
    "                    elif 'llama3' in name.lower():\n",
    "                        print(f\"   ðŸ¦™ {name} ({size_gb:.1f} GB) â† Good general model\")\n",
    "                    else:\n",
    "                        print(f\"   â€¢ {name} ({size_gb:.1f} GB)\")\n",
    "                \n",
    "                print(\"\\nðŸ’¡ Recommendation:\")\n",
    "                danish_models = [m for m in models if 'danish' in m.get('name', '').lower() or 'openeuro' in m.get('name', '').lower()]\n",
    "                \n",
    "                if danish_models:\n",
    "                    print(f\"   Use: '{danish_models[0]['name']}' (Danish-optimized)\")\n",
    "                elif any('llama3' in m.get('name', '').lower() for m in models):\n",
    "                    llama3 = [m for m in models if 'llama3' in m.get('name', '').lower()][0]\n",
    "                    print(f\"   Use: '{llama3['name']}' (general purpose)\")\n",
    "                else:\n",
    "                    print(f\"   Use: '{models[0]['name']}'\")\n",
    "                \n",
    "                return True, models\n",
    "            else:\n",
    "                print(\"âš ï¸  Ollama is running but no models are installed!\")\n",
    "                print(\"\\nðŸ“¥ Install a model:\")\n",
    "                print(\"   For Danish: ollama pull jobautomation/OpenEuroLLM-Danish:latest\")\n",
    "                print(\"   General:    ollama pull llama3.1\")\n",
    "                return False, []\n",
    "        else:\n",
    "            print(f\"âŒ Ollama responded with error: {response.status_code}\")\n",
    "            return False, []\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama!\")\n",
    "        print(\"\\nðŸ”§ To fix:\")\n",
    "        print(\"   1. Install Ollama: https://ollama.ai\")\n",
    "        print(\"   2. Start Ollama: ollama serve\")\n",
    "        print(\"   3. Pull a model: ollama pull llama3.1\")\n",
    "        return False, []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking Ollama: {e}\")\n",
    "        return False, []\n",
    "\n",
    "# Run the check\n",
    "ollama_ok, available_models = check_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c09f327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Transcripts: D:\\legendary2\\transcribe\n",
      "ðŸ“‹ Recipe: C:\\dev\\qualvalt\\projects\\Legundary\\processing_recipe.yaml\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Configuration\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from qualivault.validation import OllamaValidator, validate_recipe_transcripts\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "config_path = project_root / \"config.yml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "recipe_path = project_root / \"processing_recipe.yaml\"\n",
    "transcripts_dir = (project_root / config['paths']['output_base_folder']).resolve()\n",
    "\n",
    "print(f\"ðŸ“‚ Transcripts: {transcripts_dir}\")\n",
    "print(f\"ðŸ“‹ Recipe: {recipe_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2934e",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control validation:\n",
    "\n",
    "- **model**: Ollama model to use (choose from available models above)\n",
    "- **sample_rate**: Fraction of segments to check (0.1 = 10%, 1.0 = 100%)\n",
    "- **language**: Expected language of transcripts\n",
    "- **ollama_url**: URL where Ollama is running (default: localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc717f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Model: llama3.1\n",
      "ðŸ“Š Sample Rate: 5.0%\n",
      "ðŸŒ Language: Danish\n",
      "ðŸ“„ Max Interviews: 3\n",
      "âš™ï¸  Max segments/file: 50\n",
      "\n",
      "âœ… Model 'llama3.1' is responding: 'Hiya'\n"
     ]
    }
   ],
   "source": [
    "# 3. Validation Settings\n",
    "# Choose model based on available models above\n",
    "MODEL = \"llama3.1\"              # Change to your preferred model\n",
    "SAMPLE_RATE = 0.05              # Default sample rate\n",
    "LANGUAGE = \"Danish\"             # Expected language\n",
    "MAX_INTERVIEWS = 3              # Limit interviews for iterative tuning (None = all)\n",
    "\n",
    "# Central place to tweak validation behavior\n",
    "VALIDATION_PARAMS = {\n",
    "    \"sample_rate\": SAMPLE_RATE,     # fraction of segments to check\n",
    "    \"min_text_length\": 8,           # skip very short fragments\n",
    "    \"max_segments\": 50,             # limit segments per file (None for all)\n",
    "    \"max_interviews\": MAX_INTERVIEWS, # limit number of interviews to process\n",
    "    \"ollama_options\": {             # passed to Ollama generate options\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    # Friendly, line-broken prompt a non-coder can edit\n",
    "    \"prompt_template\": \"\"\"\n",
    "You are validating a {language} interview transcript segment.\n",
    "\n",
    "Segment: \"{text}\"\n",
    "Speaker: {speaker}\n",
    "\n",
    "Analyze this segment and identify:\n",
    "1. Hallucinations (repeated phrases, gibberish, random text)\n",
    "2. Transcription errors (misheard words that don't fit context)\n",
    "3. Audio artifacts (timestamps, file names, subtitles metadata)\n",
    "4. Incomplete sentences or fragments\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"has_issues\": true/false,\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"issues\": [\"issue1\", \"issue2\"],\n",
    "    \"suggestions\": \"brief correction suggestion\"\n",
    "}}\n",
    "\n",
    "Be conservative - only flag clear problems.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"ðŸ¤– Model: {MODEL}\")\n",
    "print(f\"ðŸ“Š Sample Rate: {VALIDATION_PARAMS['sample_rate'] * 100}%\")\n",
    "print(f\"ðŸŒ Language: {LANGUAGE}\")\n",
    "print(f\"ðŸ“„ Max Interviews: {VALIDATION_PARAMS['max_interviews'] or 'All'}\")\n",
    "print(f\"âš™ï¸  Max segments/file: {VALIDATION_PARAMS['max_segments']}\")\n",
    "\n",
    "# Quick test\n",
    "if ollama_ok:\n",
    "    validator = OllamaValidator(model=MODEL, ollama_url=OLLAMA_URL)\n",
    "    test_response = validator._query_ollama(\"Say 'Hello' in one word.\", options=VALIDATION_PARAMS.get(\"ollama_options\"))\n",
    "    \n",
    "    if test_response:\n",
    "        print(f\"\\nâœ… Model '{MODEL}' is responding: '{test_response.strip()[:50]}'\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Model '{MODEL}' is not responding. Check if it's installed.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Skipping test - Ollama not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b2046",
   "metadata": {},
   "source": [
    "## Validate Transcripts\n",
    "\n",
    "Run validation on all transcripts in the recipe. This will:\n",
    "- Sample segments from each transcript\n",
    "- Check for hallucinations, repeated words, and inconsistencies\n",
    "- Generate a detailed report with flagged segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbdb3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Starting validation at 12:35:56\n",
      "   Model: llama3.1\n",
      "   Sample Rate: 5.0%\n",
      "   Max Interviews: 3\n",
      "   Scanning: D:\\legendary2\\transcribe\n",
      "======================================================================\n",
      "Found 3 CSV files\n",
      "\n",
      "[1/3] Validating Interview_1.csv... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error querying Ollama: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 7 issues in 8 segments\n",
      "[2/3] Validating Interview_10.csv... âœ… 11 issues in 11 segments\n",
      "[3/3] Validating Interview_102.csv... âœ… 16 issues in 17 segments\n",
      "\n",
      "======================================================================\n",
      "âœ… Validation complete at 12:41:21\n",
      "   Total transcripts processed: 3\n"
     ]
    }
   ],
   "source": [
    "# 4. Run Validation\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"ðŸ” Starting validation at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"   Model: {MODEL}\")\n",
    "print(f\"   Sample Rate: {VALIDATION_PARAMS['sample_rate'] * 100}%\")\n",
    "print(f\"   Max Interviews: {VALIDATION_PARAMS['max_interviews'] or 'All'}\")\n",
    "print(f\"   Scanning: {transcripts_dir}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find all CSV files to validate (don't depend on recipe status)\n",
    "csv_files = sorted(transcripts_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Limit interviews if specified\n",
    "if VALIDATION_PARAMS.get(\"max_interviews\"):\n",
    "    csv_files = csv_files[:VALIDATION_PARAMS[\"max_interviews\"]]\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"âŒ No CSV files found in {transcripts_dir}\")\n",
    "    results = []\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        print(f\"[{i}/{len(csv_files)}] Validating {csv_file.name}...\", end=\" \")\n",
    "        \n",
    "        validator = OllamaValidator(model=MODEL, ollama_url=OLLAMA_URL)\n",
    "        report = validator.validate_transcript(\n",
    "            csv_file,\n",
    "            sample_rate=VALIDATION_PARAMS[\"sample_rate\"],\n",
    "            language=LANGUAGE,\n",
    "            validation_params=VALIDATION_PARAMS\n",
    "        )\n",
    "        \n",
    "        if report:\n",
    "            results.append(report)\n",
    "            flagged = len(report.get('flagged_segments', []))\n",
    "            checked = report.get('segments_checked', 0)\n",
    "            print(f\"âœ… {flagged} issues in {checked} segments\")\n",
    "        else:\n",
    "            print(f\"â­ï¸  Skipped\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"âœ… Validation complete at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"   Total transcripts processed: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52563b91",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Overview of validation results across all transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db9deb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š VALIDATION SUMMARY\n",
      "======================================================================\n",
      "Files validated: 3\n",
      "Total segments checked: 36\n",
      "Total segments flagged: 34\n",
      "Flag rate: 94.4%\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Per-File Results:\n",
      "         File  Checked  Flagged   Rate\n",
      "  Interview_1        8        7  87.5%\n",
      " Interview_10       11       11 100.0%\n",
      "Interview_102       17       16  94.1%\n"
     ]
    }
   ],
   "source": [
    "# 5. Display Summary\n",
    "import pandas as pd\n",
    "\n",
    "# Pull totals from results\n",
    "total_flagged = sum(r.get('flagged_count', len(r.get('flagged_segments', []))) for r in results)\n",
    "total_checked = sum(r.get('segments_checked', 0) for r in results)\n",
    "\n",
    "print(f\"ðŸ“Š VALIDATION SUMMARY\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"Files validated: {len(results)}\")\n",
    "print(f\"Total segments checked: {total_checked}\")\n",
    "print(f\"Total segments flagged: {total_flagged}\")\n",
    "if total_checked > 0:\n",
    "    print(f\"Flag rate: {(total_flagged / total_checked * 100):.1f}%\")\n",
    "print(f\"=\" * 70)\n",
    "\n",
    "# Per-file breakdown\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'File': Path(r.get('csv_file', 'unknown')).stem,\n",
    "        'Checked': r.get('segments_checked', 0),\n",
    "        'Flagged': r.get('flagged_count', len(r.get('flagged_segments', []))),\n",
    "        'Rate': f\"{(r.get('flagged_count', len(r.get('flagged_segments', []))) / max(r.get('segments_checked', 1), 1) * 100):.1f}%\" \n",
    "               if r.get('segments_checked', 0) > 0 else \"N/A\"\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print(\"\\nðŸ“ Per-File Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8e9f4",
   "metadata": {},
   "source": [
    "## Common Transcription Errors\n",
    "\n",
    "Domain-specific errors and suspicious patterns found across all transcripts.\n",
    "Use this list to create auto-correction rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725a6a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  MOST COMMON ERROR TYPES\n",
      "======================================================================\n",
      "Error Type                           | Count | %\n",
      "----------------------------------------------------------------------\n",
      " 1. Transcription error            |   28 |  42.4%\n",
      " 2. Incomplete sentence or frag... |   20 |  30.3%\n",
      " 3. {'type'                        |    9 |  13.6%\n",
      " 4. Incomplete sentence            |    4 |   6.1%\n",
      " 5. Incomplete sentence/fragment   |    3 |   4.5%\n",
      " 6. Hallucinations                 |    1 |   1.5%\n",
      " 7. Transcription errors           |    1 |   1.5%\n",
      "\n",
      "ðŸ’¡ Use these patterns to create auto-correction rules or improve the Ollama prompt\n"
     ]
    }
   ],
   "source": [
    "# 6a. Extract Common Errors (Systemic Issues - by TYPE)\n",
    "from qualivault.validation import extract_common_errors\n",
    "\n",
    "common_errors = extract_common_errors(results, top_n=15)\n",
    "\n",
    "if common_errors:\n",
    "    print(\"âš ï¸  MOST COMMON ERROR TYPES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Error Type                           | Count | %\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Calculate total for percentages\n",
    "    total_errors = sum(e['count'] for e in common_errors)\n",
    "    \n",
    "    for i, error_info in enumerate(common_errors, 1):\n",
    "        error_type = error_info['error_type']\n",
    "        count = error_info['count']\n",
    "        percentage = (count / total_errors * 100) if total_errors > 0 else 0\n",
    "        \n",
    "        # Truncate long error types\n",
    "        display_type = error_type[:30] if len(error_type) <= 30 else error_type[:27] + \"...\"\n",
    "        \n",
    "        print(f\"{i:2d}. {display_type:<30} | {count:>4d} | {percentage:>5.1f}%\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Use these patterns to create auto-correction rules or improve the Ollama prompt\")\n",
    "else:\n",
    "    print(\"âœ… No common errors identified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa76f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  MOST COMMON TRANSCRIPTION ERRORS\n",
      "======================================================================\n",
      " Rank                      error_type  count\n",
      "    1             Transcription error     28\n",
      "    2 Incomplete sentence or fragment     20\n",
      "    3                         {'type'      9\n",
      "    4             Incomplete sentence      4\n",
      "    5    Incomplete sentence/fragment      3\n",
      "    6                  Hallucinations      1\n",
      "    7            Transcription errors      1\n",
      "\n",
      "ðŸ’¡ Use these to create auto-correction rules or adjust Ollama prompt\n"
     ]
    }
   ],
   "source": [
    "# 6a. Extract Common Errors (Systemic Issues)\n",
    "from qualivault.validation import extract_common_errors\n",
    "\n",
    "common_errors = extract_common_errors(results, top_n=15)\n",
    "\n",
    "if common_errors:\n",
    "    print(\"âš ï¸  MOST COMMON TRANSCRIPTION ERRORS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create DataFrame for nice display\n",
    "    error_df = pd.DataFrame(common_errors)\n",
    "    \n",
    "    # Add rank column\n",
    "    error_df.insert(0, 'Rank', range(1, len(error_df) + 1))\n",
    "    \n",
    "    print(error_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Use these to create auto-correction rules or adjust Ollama prompt\")\n",
    "else:\n",
    "    print(\"âœ… No common errors identified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855c7cd",
   "metadata": {},
   "source": [
    "## Export Report\n",
    "\n",
    "Save validation results to JSON for later review or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07833b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Report saved to: C:\\dev\\qualvalt\\projects\\Legundary\\validation_report.json\n",
      "   Size: 27.6 KB\n"
     ]
    }
   ],
   "source": [
    "# 7. Export Results\n",
    "import json\n",
    "\n",
    "report_path = project_root / \"validation_report.json\"\n",
    "\n",
    "export_data = {\n",
    "    'validation_date': datetime.now().isoformat(),\n",
    "    'model': MODEL,\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'language': LANGUAGE,\n",
    "    'summary': {\n",
    "        'files_validated': len(results),\n",
    "        'total_segments_checked': total_checked,\n",
    "        'total_segments_flagged': total_flagged,\n",
    "        'flag_rate': f\"{(total_flagged / total_checked * 100):.1f}%\" if total_checked > 0 else \"0%\"\n",
    "    },\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ðŸ’¾ Report saved to: {report_path}\")\n",
    "print(f\"   Size: {report_path.stat().st_size / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualivault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
