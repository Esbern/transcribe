{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b3cfa8",
   "metadata": {},
   "source": [
    "# üîç QualiVault: Validate Transcripts\n",
    "**Goal:** Use Ollama (local LLM) to detect transcription errors and hallucinations.\n",
    "\n",
    "1. Loads transcripts from CSV files.\n",
    "2. Samples segments and sends them to Ollama for validation.\n",
    "3. Flags potential errors: hallucinations, misheard words, artifacts.\n",
    "4. Generates validation report with suggestions.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ollama must be installed and running (`ollama serve`)\n",
    "- Install a model: `ollama pull llama3.1` or `ollama pull jobautomation/OpenEuroLLM-Danish:latest`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec71826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "‚ùå Project not found: c:\\dev\\qualvalt\\projects\\YOUR_PROJECT_NAME\n   Available projects in c:\\dev\\qualvalt\\projects:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Verify project exists\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m project_root.exists():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Project not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m   Available projects in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkspace_root\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mprojects\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m config_path = project_root / \u001b[33m'\u001b[39m\u001b[33mconfig.yml\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path.exists():\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ‚ùå Project not found: c:\\dev\\qualvalt\\projects\\YOUR_PROJECT_NAME\n   Available projects in c:\\dev\\qualvalt\\projects:"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from qualivault.validation import OllamaValidator, validate_recipe_transcripts\n",
    "\n",
    "# ============================================\n",
    "# PROJECT CONFIGURATION\n",
    "# ============================================\n",
    "# Specify your project folder name here:\n",
    "PROJECT_NAME = 'YOUR_PROJECT_NAME'  # <-- Change this to your project folder name\n",
    "\n",
    "# Auto-detect workspace root and project path\n",
    "workspace_root = Path(r'c:\\dev\\qualvalt')  # Workspace root\n",
    "project_root = workspace_root / 'projects' / PROJECT_NAME\n",
    "\n",
    "# Verify project exists\n",
    "if not project_root.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Project not found: {project_root}\\n   Available projects in {workspace_root / 'projects'}:\")\n",
    "    \n",
    "config_path = project_root / 'config.yml'\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Config not found: {config_path}\")\n",
    "\n",
    "print(f\"üéØ Working on project: {PROJECT_NAME}\")\n",
    "print(f\"üìÅ Project root:       {project_root}\")\n",
    "print(f\"‚öôÔ∏è  Config file:        {config_path}\")\n",
    "print()\n",
    "\n",
    "# 1. Load Configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "recipe_path = project_root / \"processing_recipe.yaml\"\n",
    "transcripts_dir = (project_root / config['paths']['output_base_folder']).resolve()\n",
    "\n",
    "print(f\"üìÇ Transcripts: {transcripts_dir}\")\n",
    "print(f\"üìã Recipe: {recipe_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check Ollama Installation and Available Models\n",
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama():\n",
    "    \"\"\"Test if Ollama is running and list available models.\"\"\"\n",
    "    print(\"üîç Checking Ollama installation...\\n\")\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Ollama is running!\")\n",
    "            \n",
    "            # List available models\n",
    "            data = response.json()\n",
    "            models = data.get('models', [])\n",
    "            \n",
    "            if models:\n",
    "                print(f\"\\nüì¶ Available models ({len(models)}):\\n\")\n",
    "                \n",
    "                for model in models:\n",
    "                    name = model.get('name', 'Unknown')\n",
    "                    size_gb = model.get('size', 0) / (1024**3)\n",
    "                    \n",
    "                    # Highlight Danish model\n",
    "                    if 'danish' in name.lower() or 'openeuro' in name.lower():\n",
    "                        print(f\"   üá©üá∞ {name} ({size_gb:.1f} GB) ‚Üê Recommended for Danish\")\n",
    "                    elif 'llama3' in name.lower():\n",
    "                        print(f\"   ü¶ô {name} ({size_gb:.1f} GB) ‚Üê Good general model\")\n",
    "                    else:\n",
    "                        print(f\"   ‚Ä¢ {name} ({size_gb:.1f} GB)\")\n",
    "                \n",
    "                print(\"\\nüí° Recommendation:\")\n",
    "                danish_models = [m for m in models if 'danish' in m.get('name', '').lower() or 'openeuro' in m.get('name', '').lower()]\n",
    "                \n",
    "                if danish_models:\n",
    "                    print(f\"   Use: '{danish_models[0]['name']}' (Danish-optimized)\")\n",
    "                elif any('llama3' in m.get('name', '').lower() for m in models):\n",
    "                    llama3 = [m for m in models if 'llama3' in m.get('name', '').lower()][0]\n",
    "                    print(f\"   Use: '{llama3['name']}' (general purpose)\")\n",
    "                else:\n",
    "                    print(f\"   Use: '{models[0]['name']}'\")\n",
    "                \n",
    "                return True, models\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Ollama is running but no models are installed!\")\n",
    "                print(\"\\nüì• Install a model:\")\n",
    "                print(\"   For Danish: ollama pull jobautomation/OpenEuroLLM-Danish:latest\")\n",
    "                print(\"   General:    ollama pull llama3.1\")\n",
    "                return False, []\n",
    "        else:\n",
    "            print(f\"‚ùå Ollama responded with error: {response.status_code}\")\n",
    "            return False, []\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama!\")\n",
    "        print(\"\\nüîß To fix:\")\n",
    "        print(\"   1. Install Ollama: https://ollama.ai\")\n",
    "        print(\"   2. Start Ollama: ollama serve\")\n",
    "        print(\"   3. Pull a model: ollama pull llama3.1\")\n",
    "        return False, []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama: {e}\")\n",
    "        return False, []\n",
    "\n",
    "# Run the check\n",
    "ollama_ok, available_models = check_ollama()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da901b57",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control validation:\n",
    "\n",
    "- **model**: Ollama model to use (`llama2`, `mistral`, `llama3`, etc.)\n",
    "- **sample_rate**: Fraction of segments to check (0.1 = 10%, 1.0 = 100%)\n",
    "- **language**: Expected language of transcripts\n",
    "- **ollama_url**: URL where Ollama is running (default: localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Settings\n",
    "# Auto-select the best available model from the list above\n",
    "if ollama_ok and available_models:\n",
    "    # Prefer Danish models, then llama3, then first available\n",
    "    danish_models = [m for m in available_models if 'danish' in m.get('name', '').lower() or 'openeuro' in m.get('name', '').lower()]\n",
    "    llama3_models = [m for m in available_models if 'llama3' in m.get('name', '').lower()]\n",
    "    \n",
    "    if danish_models:\n",
    "        MODEL = danish_models[0]['name']\n",
    "    elif llama3_models:\n",
    "        MODEL = llama3_models[0]['name']\n",
    "    else:\n",
    "        MODEL = available_models[0]['name']\n",
    "else:\n",
    "    MODEL = \"llama3.1\"  # Fallback if check didn't run\n",
    "\n",
    "SAMPLE_RATE = 0.1           # Check 10% of segments (faster, set to 1.0 for 100%)\n",
    "LANGUAGE = \"Danish\"         # Expected language\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "TIMEOUT = 120               # Timeout in seconds (increase for large models)\n",
    "\n",
    "VALIDATION_PARAMS = {\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "    \"min_text_length\": 8,\n",
    "    \"max_segments\": None,\n",
    "    \"timeout\": TIMEOUT,\n",
    "    \"ollama_options\": {\"temperature\": 0.3, \"top_p\": 0.9}\n",
    "}\n",
    "\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üìä Sample Rate: {SAMPLE_RATE * 100}%\")\n",
    "print(f\"üåç Language: {LANGUAGE}\")\n",
    "print(f\"‚è±Ô∏è  Timeout: {TIMEOUT}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d5f84",
   "metadata": {},
   "source": [
    "## Test Ollama Connection\n",
    "\n",
    "Make sure Ollama is running before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = OllamaValidator(model=MODEL, ollama_url=OLLAMA_URL)\n",
    "\n",
    "# Quick test\n",
    "test_response = validator._query_ollama(\"Say 'Hello' in one word.\")\n",
    "if test_response:\n",
    "    print(f\"‚úÖ Ollama is responding: '{test_response.strip()[:50]}'\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama is not responding. Make sure it's running: `ollama serve`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8afc0",
   "metadata": {},
   "source": [
    "## Validate All Transcripts\n",
    "\n",
    "This will:\n",
    "1. Load all transcribed interviews from the recipe\n",
    "2. Sample segments from each CSV\n",
    "3. Check each segment with Ollama for errors\n",
    "4. Save individual validation report for each interview\n",
    "5. Create summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a42e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation on all transcripts - Save individual report for each interview\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"üîç Starting validation at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"   Model: {MODEL}\")\n",
    "print(f\"   Scanning: {transcripts_dir}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create validation reports directory\n",
    "reports_dir = project_root / \"validation_reports\"\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "print(f\"üìÅ Reports saved to: {reports_dir}\\n\")\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = sorted(transcripts_dir.glob(\"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"‚ùå No CSV files found\")\n",
    "    results = []\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "    results = []\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        interview_name = csv_file.stem\n",
    "        report_file = reports_dir / f\"{interview_name}_validation.json\"\n",
    "        \n",
    "        print(f\"[{i}/{len(csv_files)}] {interview_name}...\", end=\" \")\n",
    "        \n",
    "        validator = OllamaValidator(model=MODEL, ollama_url=OLLAMA_URL, timeout=TIMEOUT)\n",
    "        report = validator.validate_transcript(\n",
    "            csv_file,\n",
    "            sample_rate=VALIDATION_PARAMS[\"sample_rate\"],\n",
    "            language=LANGUAGE,\n",
    "            validation_params=VALIDATION_PARAMS\n",
    "        )\n",
    "        \n",
    "        if report:\n",
    "            results.append(report)\n",
    "            \n",
    "            # Save individual report\n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            flagged = len(report.get('flagged_segments', []))\n",
    "            checked = report.get('segments_checked', 0)\n",
    "            print(f\"‚úÖ {flagged}/{checked} segments\")\n",
    "        else:\n",
    "            print(f\"‚è≠Ô∏è  Skipped\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚úÖ Validation complete at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"   Reports saved to: {reports_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7eb72",
   "metadata": {},
   "source": [
    "## Review Validation Reports\n",
    "\n",
    "Inspect flagged segments and issues across all transcripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics - Load all individual reports and aggregate\n",
    "import pandas as pd\n",
    "\n",
    "# Load all individual validation reports\n",
    "reports_dir = project_root / \"validation_reports\"\n",
    "report_files = sorted(reports_dir.glob(\"*_validation.json\"))\n",
    "\n",
    "print(f\"üìÇ Loading {len(report_files)} validation reports\\n\")\n",
    "\n",
    "all_results = []\n",
    "for report_file in report_files:\n",
    "    try:\n",
    "        with open(report_file, 'r', encoding='utf-8') as f:\n",
    "            report = json.load(f)\n",
    "            all_results.append(report)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not load {report_file.name}: {e}\")\n",
    "\n",
    "# Aggregate statistics\n",
    "total_flagged = sum(r.get('flagged_count', len(r.get('flagged_segments', []))) for r in all_results)\n",
    "total_checked = sum(r.get('segments_checked', 0) for r in all_results)\n",
    "\n",
    "print(f\"üìä VALIDATION SUMMARY\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"Files validated: {len(all_results)}\")\n",
    "print(f\"Total segments checked: {total_checked}\")\n",
    "print(f\"Total segments flagged: {total_flagged}\")\n",
    "if total_checked > 0:\n",
    "    print(f\"Flag rate: {(total_flagged / total_checked * 100):.1f}%\")\n",
    "print(f\"=\" * 70)\n",
    "\n",
    "# Per-file breakdown\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'File': Path(r.get('csv_file', 'unknown')).stem,\n",
    "        'Checked': r.get('segments_checked', 0),\n",
    "        'Flagged': r.get('flagged_count', len(r.get('flagged_segments', []))),\n",
    "        'Rate': f\"{(r.get('flagged_count', len(r.get('flagged_segments', []))) / max(r.get('segments_checked', 1), 1) * 100):.1f}%\" \n",
    "               if r.get('segments_checked', 0) > 0 else \"N/A\"\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "print(\"\\nüìÅ Per-File Results:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_file = reports_dir / \"SUMMARY.csv\"\n",
    "df.to_csv(summary_file, index=False)\n",
    "print(f\"\\nüíæ Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b1215",
   "metadata": {},
   "source": [
    "## Detailed Issue Review\n",
    "\n",
    "Examine specific flagged segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5243e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed issues for first transcript\n",
    "if reports and reports[0].get('flagged_segments'):\n",
    "    report = reports[0]\n",
    "    print(f\"\\nüîç Detailed issues for: {report['csv_file']}\\n\")\n",
    "    \n",
    "    for seg in report['flagged_segments'][:10]:  # Show first 10\n",
    "        print(f\"Segment {seg['segment_index']} ({seg['start']:.1f}s - {seg['end']:.1f}s)\")\n",
    "        print(f\"Speaker: {seg['speaker']}\")\n",
    "        print(f\"Text: {seg['text']}\")\n",
    "        print(f\"Issues: {', '.join(seg['issues'])}\")\n",
    "        print(f\"Confidence: {seg['confidence']:.2f}\")\n",
    "        if seg.get('suggestions'):\n",
    "            print(f\"Suggestions: {seg['suggestions']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No issues found in first transcript.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30dfb1d",
   "metadata": {},
   "source": [
    "## Export Validation Report\n",
    "\n",
    "Create master summary file with links to individual validation reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Master Summary Report\n",
    "reports_dir = project_root / \"validation_reports\"\n",
    "\n",
    "# Create master summary\n",
    "master_summary = {\n",
    "    'validation_date': datetime.now().isoformat(),\n",
    "    'model': MODEL,\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'language': LANGUAGE,\n",
    "    'summary': {\n",
    "        'total_files_validated': len(all_results),\n",
    "        'total_segments_checked': total_checked,\n",
    "        'total_segments_flagged': total_flagged,\n",
    "        'flag_rate': f\"{(total_flagged / total_checked * 100):.1f}%\" if total_checked > 0 else \"0%\"\n",
    "    },\n",
    "    'per_file_reports': [\n",
    "        {\n",
    "            'file': Path(r.get('csv_file', 'unknown')).name,\n",
    "            'segments_checked': r.get('segments_checked', 0),\n",
    "            'segments_flagged': r.get('flagged_count', len(r.get('flagged_segments', []))),\n",
    "            'report_file': f\"{Path(r.get('csv_file', 'unknown')).stem}_validation.json\"\n",
    "        }\n",
    "        for r in all_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save master summary\n",
    "summary_file = reports_dir / \"VALIDATION_SUMMARY.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(master_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üìã Master Summary Report\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"üìÅ Location: {summary_file}\")\n",
    "print(f\"   ‚Ä¢ Master summary: {summary_file.name}\")\n",
    "print(f\"   ‚Ä¢ Individual reports: {len(list(reports_dir.glob('*_validation.json')))} files\")\n",
    "print(f\"   ‚Ä¢ Summary CSV: SUMMARY.csv\")\n",
    "print(f\"\\n‚úÖ Organization:\")\n",
    "print(f\"   {reports_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ VALIDATION_SUMMARY.json  (master overview)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ SUMMARY.csv              (per-file table)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Interview_1_validation.json\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Interview_2_validation.json\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualivault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
