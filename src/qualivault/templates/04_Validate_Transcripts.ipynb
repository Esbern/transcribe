{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b3cfa8",
   "metadata": {},
   "source": [
    "# üîç QualiVault: Validate Transcripts\n",
    "**Goal:** Use Ollama (local LLM) to detect transcription errors and hallucinations.\n",
    "\n",
    "1. Loads transcripts from CSV files.\n",
    "2. Samples segments and sends them to Ollama for validation.\n",
    "3. Flags potential errors: hallucinations, misheard words, artifacts.\n",
    "4. Generates validation report with suggestions.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ollama must be installed and running (`ollama serve`)\n",
    "- Install a model: `ollama pull llama3.1` or `ollama pull jobautomation/OpenEuroLLM-Danish:latest`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "üéØ Working on project: Legundary\n",
      "üìÅ Project root:       c:\\dev\\qualvalt\\projects\\Legundary\n",
      "‚öôÔ∏è  Config file:        c:\\dev\\qualvalt\\projects\\Legundary\\config.yml\n",
      "\n",
      "üìÇ Transcripts: D:\\legendary2\\transcribe\n",
      "üìã Recipe: c:\\dev\\qualvalt\\projects\\Legundary\\processing_recipe.yaml\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from qualivault.validation import OllamaValidator, validate_recipe_transcripts\n",
    "\n",
    "# ============================================\n",
    "# PROJECT CONFIGURATION\n",
    "# ============================================\n",
    "# Specify your project folder name here:\n",
    "PROJECT_NAME = 'YOUR_PROJECT_NAME'  # <-- Change this to your project folder name\n",
    "\n",
    "# Auto-detect workspace root and project path\n",
    "workspace_root = Path(r'c:\\dev\\qualvalt')  # Workspace root\n",
    "project_root = workspace_root / 'projects' / PROJECT_NAME\n",
    "\n",
    "# Verify project exists\n",
    "if not project_root.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Project not found: {project_root}\\n   Available projects in {workspace_root / 'projects'}:\")\n",
    "    \n",
    "config_path = project_root / 'config.yml'\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Config not found: {config_path}\")\n",
    "\n",
    "print(f\"üéØ Working on project: {PROJECT_NAME}\")\n",
    "print(f\"üìÅ Project root:       {project_root}\")\n",
    "print(f\"‚öôÔ∏è  Config file:        {config_path}\")\n",
    "print()\n",
    "\n",
    "# 1. Load Configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "recipe_path = project_root / \"processing_recipe.yaml\"\n",
    "transcripts_dir = (project_root / config['paths']['output_base_folder']).resolve()\n",
    "\n",
    "print(f\"üìÇ Transcripts: {transcripts_dir}\")\n",
    "print(f\"üìã Recipe: {recipe_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check Ollama Installation and Available Models\n",
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama():\n",
    "    \"\"\"Test if Ollama is running and list available models.\"\"\"\n",
    "    print(\"üîç Checking Ollama installation...\\n\")\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Ollama is running!\")\n",
    "            \n",
    "            # List available models\n",
    "            data = response.json()\n",
    "            models = data.get('models', [])\n",
    "            \n",
    "            if models:\n",
    "                print(f\"\\nüì¶ Available models ({len(models)}):\\n\")\n",
    "                \n",
    "                for model in models:\n",
    "                    name = model.get('name', 'Unknown')\n",
    "                    size_gb = model.get('size', 0) / (1024**3)\n",
    "                    \n",
    "                    # Highlight Danish model\n",
    "                    if 'danish' in name.lower() or 'openeuro' in name.lower():\n",
    "                        print(f\"   üá©üá∞ {name} ({size_gb:.1f} GB) ‚Üê Recommended for Danish\")\n",
    "                    elif 'llama3' in name.lower():\n",
    "                        print(f\"   ü¶ô {name} ({size_gb:.1f} GB) ‚Üê Good general model\")\n",
    "                    else:\n",
    "                        print(f\"   ‚Ä¢ {name} ({size_gb:.1f} GB)\")\n",
    "                \n",
    "                print(\"\\nüí° Recommendation:\")\n",
    "                danish_models = [m for m in models if 'danish' in m.get('name', '').lower() or 'openeuro' in m.get('name', '').lower()]\n",
    "                \n",
    "                if danish_models:\n",
    "                    print(f\"   Use: '{danish_models[0]['name']}' (Danish-optimized)\")\n",
    "                elif any('llama3' in m.get('name', '').lower() for m in models):\n",
    "                    llama3 = [m for m in models if 'llama3' in m.get('name', '').lower()][0]\n",
    "                    print(f\"   Use: '{llama3['name']}' (general purpose)\")\n",
    "                else:\n",
    "                    print(f\"   Use: '{models[0]['name']}'\")\n",
    "                \n",
    "                return True, models\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Ollama is running but no models are installed!\")\n",
    "                print(\"\\nüì• Install a model:\")\n",
    "                print(\"   For Danish: ollama pull jobautomation/OpenEuroLLM-Danish:latest\")\n",
    "                print(\"   General:    ollama pull llama3.1\")\n",
    "                return False, []\n",
    "        else:\n",
    "            print(f\"‚ùå Ollama responded with error: {response.status_code}\")\n",
    "            return False, []\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama!\")\n",
    "        print(\"\\nüîß To fix:\")\n",
    "        print(\"   1. Install Ollama: https://ollama.ai\")\n",
    "        print(\"   2. Start Ollama: ollama serve\")\n",
    "        print(\"   3. Pull a model: ollama pull llama3.1\")\n",
    "        return False, []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama: {e}\")\n",
    "        return False, []\n",
    "\n",
    "# Run the check\n",
    "ollama_ok, available_models = check_ollama()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da901b57",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control validation:\n",
    "\n",
    "- **model**: Ollama model to use (`llama2`, `mistral`, `llama3`, etc.)\n",
    "- **sample_rate**: Fraction of segments to check (0.1 = 10%, 1.0 = 100%)\n",
    "- **language**: Expected language of transcripts\n",
    "- **ollama_url**: URL where Ollama is running (default: localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model: llama2\n",
      "üìä Sample Rate: 100%\n",
      "üåç Language: Danish\n"
     ]
    }
   ],
   "source": [
    "# Validation Settings\n",
    "# Auto-select the best available model from the list above\n",
    "if ollama_ok and available_models:\n",
    "    # Prefer Danish models, then llama3, then first available\n",
    "    danish_models = [m for m in available_models if 'danish' in m.get('name', '').lower() or 'openeuro' in m.get('name', '').lower()]\n",
    "    llama3_models = [m for m in available_models if 'llama3' in m.get('name', '').lower()]\n",
    "    \n",
    "    if danish_models:\n",
    "        MODEL = danish_models[0]['name']\n",
    "    elif llama3_models:\n",
    "        MODEL = llama3_models[0]['name']\n",
    "    else:\n",
    "        MODEL = available_models[0]['name']\n",
    "else:\n",
    "    MODEL = \"llama3.1\"  # Fallback if check didn't run\n",
    "\n",
    "SAMPLE_RATE = 0.1           # Check 10% of segments (faster, set to 1.0 for 100%)\n",
    "LANGUAGE = \"Danish\"         # Expected language\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üìä Sample Rate: {SAMPLE_RATE * 100}%\")\n",
    "print(f\"üåç Language: {LANGUAGE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d5f84",
   "metadata": {},
   "source": [
    "## Test Ollama Connection\n",
    "\n",
    "Make sure Ollama is running before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801d321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ollama API error: 404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Ollama is not responding. Make sure it's running: `ollama serve`\n"
     ]
    }
   ],
   "source": [
    "validator = OllamaValidator(model=MODEL, ollama_url=OLLAMA_URL)\n",
    "\n",
    "# Quick test\n",
    "test_response = validator._query_ollama(\"Say 'Hello' in one word.\")\n",
    "if test_response:\n",
    "    print(f\"‚úÖ Ollama is responding: '{test_response.strip()[:50]}'\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama is not responding. Make sure it's running: `ollama serve`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8afc0",
   "metadata": {},
   "source": [
    "## Validate All Transcripts\n",
    "\n",
    "This will:\n",
    "1. Load all transcribed interviews from the recipe\n",
    "2. Sample segments from each CSV\n",
    "3. Check each segment with Ollama for errors\n",
    "4. Generate validation reports\n",
    "5. Update recipe with validation status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a42e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation on all transcripts\n",
    "reports = validate_recipe_transcripts(\n",
    "    recipe_path=recipe_path,\n",
    "    transcripts_dir=transcripts_dir,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    model=MODEL,\n",
    "    language=LANGUAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7eb72",
   "metadata": {},
   "source": [
    "## Review Validation Reports\n",
    "\n",
    "Inspect flagged segments and issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "import pandas as pd\n",
    "\n",
    "if reports:\n",
    "    print(f\"\\nüìä Validation Summary:\")\n",
    "    print(f\"   Total transcripts validated: {len(reports)}\")\n",
    "    \n",
    "    total_flagged = sum(r.get('flagged_count', 0) for r in reports)\n",
    "    print(f\"   Total issues flagged: {total_flagged}\")\n",
    "    \n",
    "    # Show transcripts with most issues\n",
    "    sorted_reports = sorted(reports, key=lambda r: r.get('flagged_count', 0), reverse=True)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Top 5 transcripts with most issues:\")\n",
    "    for i, report in enumerate(sorted_reports[:5], 1):\n",
    "        print(f\"   {i}. {report['csv_file']}: {report['flagged_count']} issues\")\n",
    "else:\n",
    "    print(\"No reports generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b1215",
   "metadata": {},
   "source": [
    "## Detailed Issue Review\n",
    "\n",
    "Examine specific flagged segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5243e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed issues for first transcript\n",
    "if reports and reports[0].get('flagged_segments'):\n",
    "    report = reports[0]\n",
    "    print(f\"\\nüîç Detailed issues for: {report['csv_file']}\\n\")\n",
    "    \n",
    "    for seg in report['flagged_segments'][:10]:  # Show first 10\n",
    "        print(f\"Segment {seg['segment_index']} ({seg['start']:.1f}s - {seg['end']:.1f}s)\")\n",
    "        print(f\"Speaker: {seg['speaker']}\")\n",
    "        print(f\"Text: {seg['text']}\")\n",
    "        print(f\"Issues: {', '.join(seg['issues'])}\")\n",
    "        print(f\"Confidence: {seg['confidence']:.2f}\")\n",
    "        if seg.get('suggestions'):\n",
    "            print(f\"Suggestions: {seg['suggestions']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No issues found in first transcript.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30dfb1d",
   "metadata": {},
   "source": [
    "## Export Validation Report\n",
    "\n",
    "Save the full validation report as JSON for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "report_file = project_root / \"validation_report.json\"\n",
    "\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(reports, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Validation report saved to: {report_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualivault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
