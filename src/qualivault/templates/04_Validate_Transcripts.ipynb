{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b3cfa8",
   "metadata": {},
   "source": [
    "# üîç QualiVault: Validate Transcripts\n",
    "**Goal:** Use Ollama (local LLM) to detect transcription errors and hallucinations.\n",
    "\n",
    "1. Loads transcripts from CSV files.\n",
    "2. Samples segments and sends them to Ollama for validation.\n",
    "3. Flags potential errors: hallucinations, misheard words, artifacts.\n",
    "4. Generates validation report with suggestions.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ollama must be installed and running (`ollama serve`)\n",
    "- Install a model: `ollama pull llama2` or `ollama pull mistral`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from qualivault.validation import OllamaValidator, validate_recipe_transcripts\n",
    "\n",
    "# 1. Load Configuration\n",
    "project_root = Path(\"..\").resolve()\n",
    "config_path = project_root / \"config.yml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "recipe_path = project_root / \"processing_recipe.yaml\"\n",
    "transcripts_dir = (project_root / config['paths']['output_base_folder']).resolve()\n",
    "\n",
    "print(f\"üìÇ Transcripts: {transcripts_dir}\")\n",
    "print(f\"üìã Recipe: {recipe_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da901b57",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control validation:\n",
    "\n",
    "- **model**: Ollama model to use (`llama2`, `mistral`, `llama3`, etc.)\n",
    "- **sample_rate**: Fraction of segments to check (0.1 = 10%, 1.0 = 100%)\n",
    "- **language**: Expected language of transcripts\n",
    "- **ollama_url**: URL where Ollama is running (default: localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Settings\n",
    "MODEL = \"llama2\"              # Ollama model to use\n",
    "SAMPLE_RATE = 0.1             # Check 10% of segments (faster)\n",
    "LANGUAGE = \"Danish\"           # Expected language\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "print(f\"üìä Sample Rate: {SAMPLE_RATE * 100}%\")\n",
    "print(f\"üåç Language: {LANGUAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d5f84",
   "metadata": {},
   "source": [
    "## Test Ollama Connection\n",
    "\n",
    "Make sure Ollama is running before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = OllamaValidator(model=MODEL, ollama_url=OLLAMA_URL)\n",
    "\n",
    "# Quick test\n",
    "test_response = validator._query_ollama(\"Say 'Hello' in one word.\")\n",
    "if test_response:\n",
    "    print(f\"‚úÖ Ollama is responding: '{test_response.strip()[:50]}'\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama is not responding. Make sure it's running: `ollama serve`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8afc0",
   "metadata": {},
   "source": [
    "## Validate All Transcripts\n",
    "\n",
    "This will:\n",
    "1. Load all transcribed interviews from the recipe\n",
    "2. Sample segments from each CSV\n",
    "3. Check each segment with Ollama for errors\n",
    "4. Generate validation reports\n",
    "5. Update recipe with validation status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a42e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation on all transcripts\n",
    "reports = validate_recipe_transcripts(\n",
    "    recipe_path=recipe_path,\n",
    "    transcripts_dir=transcripts_dir,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    model=MODEL,\n",
    "    language=LANGUAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7eb72",
   "metadata": {},
   "source": [
    "## Review Validation Reports\n",
    "\n",
    "Inspect flagged segments and issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "import pandas as pd\n",
    "\n",
    "if reports:\n",
    "    print(f\"\\nüìä Validation Summary:\")\n",
    "    print(f\"   Total transcripts validated: {len(reports)}\")\n",
    "    \n",
    "    total_flagged = sum(r.get('flagged_count', 0) for r in reports)\n",
    "    print(f\"   Total issues flagged: {total_flagged}\")\n",
    "    \n",
    "    # Show transcripts with most issues\n",
    "    sorted_reports = sorted(reports, key=lambda r: r.get('flagged_count', 0), reverse=True)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Top 5 transcripts with most issues:\")\n",
    "    for i, report in enumerate(sorted_reports[:5], 1):\n",
    "        print(f\"   {i}. {report['csv_file']}: {report['flagged_count']} issues\")\n",
    "else:\n",
    "    print(\"No reports generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b1215",
   "metadata": {},
   "source": [
    "## Detailed Issue Review\n",
    "\n",
    "Examine specific flagged segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5243e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed issues for first transcript\n",
    "if reports and reports[0].get('flagged_segments'):\n",
    "    report = reports[0]\n",
    "    print(f\"\\nüîç Detailed issues for: {report['csv_file']}\\n\")\n",
    "    \n",
    "    for seg in report['flagged_segments'][:10]:  # Show first 10\n",
    "        print(f\"Segment {seg['segment_index']} ({seg['start']:.1f}s - {seg['end']:.1f}s)\")\n",
    "        print(f\"Speaker: {seg['speaker']}\")\n",
    "        print(f\"Text: {seg['text']}\")\n",
    "        print(f\"Issues: {', '.join(seg['issues'])}\")\n",
    "        print(f\"Confidence: {seg['confidence']:.2f}\")\n",
    "        if seg.get('suggestions'):\n",
    "            print(f\"Suggestions: {seg['suggestions']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No issues found in first transcript.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30dfb1d",
   "metadata": {},
   "source": [
    "## Export Validation Report\n",
    "\n",
    "Save the full validation report as JSON for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "report_file = project_root / \"validation_report.json\"\n",
    "\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(reports, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Validation report saved to: {report_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
